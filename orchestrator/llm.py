"""LLM integration helpers for the accord orchestrator."""
from __future__ import annotations

import logging
import os
import time
from dataclasses import dataclass
from typing import Sequence


LOGGER = logging.getLogger(__name__)


class LLMConfigurationError(RuntimeError):
    """Raised when an LLM provider cannot be configured."""


@dataclass
class GenerateRequest:
    agent_id: str
    prompt: str
    context: Sequence[str]
    knowledge_refs: Sequence[str]


class LLMClient:
    """Dispatches to the configured LLM provider.

    Supported providers:
    - ``mock`` (default): deterministic local generator useful for testing.
    - ``openai``: OpenAI Responses API (requires ``openai`` package and
      ``OPENAI_API_KEY`` environment variable).
    """

    def __init__(self) -> None:
        self.provider = os.environ.get("ACCORD_LLM_PROVIDER", "mock").lower()
        self.openai_model = os.environ.get("ACCORD_OPENAI_MODEL", "gpt-4o-mini")

    def generate(self, request: GenerateRequest) -> str:
        if self.provider == "mock":
            return self._generate_mock(request)
        if self.provider == "openai":
            return self._generate_openai(request)
        raise LLMConfigurationError(f"Unsupported LLM provider: {self.provider}")

    # Providers -----------------------------------------------------------------
    def _generate_mock(self, request: GenerateRequest) -> str:
        header = f"# Draft generated for {request.agent_id}\n\n"
        context_block = "\n\n".join(snippet[:800] for snippet in request.context)
        knowledge_block = (
            "\n\n## Knowledge References\n" + "\n".join(f"- {ref}" for ref in request.knowledge_refs)
            if request.knowledge_refs
            else ""
        )
        body = (
            "This draft was generated by the mock LLM provider. Replace the provider\n"
            "with `ACCORD_LLM_PROVIDER=openai` (and set `OPENAI_API_KEY`) to obtain\n"
            "model-backed content.\n"
        )
        return header + body + knowledge_block + "\n\n--- Context Preview ---\n" + context_block

    def _generate_openai(self, request: GenerateRequest) -> str:
        try:
            from openai import OpenAI, RateLimitError  # type: ignore
        except ModuleNotFoundError as exc:  # pragma: no cover - imported lazily
            raise LLMConfigurationError(
                "openai provider selected but 'openai' package is not installed"
            ) from exc

        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise LLMConfigurationError("OPENAI_API_KEY is not set")

        client = OpenAI(api_key=api_key)
        system_prompt = (
            "You are an agent participating in the accord multi-agent organization."
            " Follow the role charter and produce actionable output with clear next steps."
        )
        context_block = "\n\n".join(snippet[:1200] for snippet in request.context)
        knowledge_block = (
            "\n\nKnowledge references:\n" + "\n".join(request.knowledge_refs)
            if request.knowledge_refs
            else ""
        )
        user_content = (
            f"Role prompt:\n{request.prompt}\n\nContext:\n{context_block}{knowledge_block}"
        )

        max_attempts = int(os.environ.get("ACCORD_OPENAI_MAX_RETRIES", 5))
        base_delay = float(os.environ.get("ACCORD_OPENAI_RETRY_DELAY", 5.0))

        last_error: Exception | None = None
        for attempt in range(1, max_attempts + 1):
            try:
                response = client.chat.completions.create(
                    model=self.openai_model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_content},
                    ],
                    temperature=float(os.environ.get("ACCORD_OPENAI_TEMPERATURE", 0.2)),
                )
                choice = response.choices[0].message
                return choice.content if isinstance(choice.content, str) else "".join(choice.content) if choice.content else ""
            except RateLimitError as exc:
                last_error = exc
                if attempt == max_attempts:
                    break
                delay = base_delay * attempt
                LOGGER.warning(
                    "OpenAI rate limit hit for %s (attempt %s/%s). Retrying in %.1fs.",
                    self.openai_model,
                    attempt,
                    max_attempts,
                    delay,
                )
                time.sleep(delay)
        raise RuntimeError("OpenAI rate limit exceeded") from last_error


__all__ = ["LLMClient", "LLMConfigurationError", "GenerateRequest"]
